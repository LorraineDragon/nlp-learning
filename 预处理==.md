如果要做文本分类聚类，分词之后的关键步骤是：向量化 或 向量化特例 Hash Trick

背景知识

# 词袋模型

Bag of Words, BoW

#### 假设

不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重，而权重与词在文本中出现的频率有关。

#### 步骤

* ##### 分词 tokenizing
* ##### 统计修订词特征值 counting

  ```
    统计每个词在文本中出现的次数，可以得到该文本基于词的特征，将词与对应的词频放在一起，即向量化。

    向量化完毕后一般会使用TF-IDF进行特征的权重修正
  ```
* ##### 标准化 normalizing

#### 局限性

仅仅考虑词频，没有考虑上下文关系，因此会丢失一部分文本的语义。

但如果目的只是分类聚类，词袋模型表现的很好。

# 词集模型

Set of Words, SoW

与词袋模型唯一的不同是，它仅考虑词是否在文本中出现，而不考虑词频。

即，一个词在文本中出现一次和多次出现，特征处理是一样的。

多数时候还是采用词袋模型。



# 特征向量化

处理后的词向量是稀疏的。

但当分词后的词汇表非常大时，需要进行特征的降维。

参考资料：[http://www.cnblogs.com/pinard/p/6688348.html](http://www.cnblogs.com/pinard/p/6688348.html)



# Hash Trick

最常用的文本降维方法。缺点是转换后的特征难以解释。

如果采用分布式计算框架，就可以避免内存不够用的情况，直接使用特征向量化方法。



# TF-IDF

Term Frequency -  Inverse Document Frequency，词频-逆文本频率

用来解决文本向量化特征不足的问题。



TF，即词频

IDF，反应了一个词在所有文本中出现的概率，如果一个词在很多文本中出现，则IDF值应该低；反之一些专业词，应该高。

对生僻词，如果语料库中美誉，一般的IDF就会没有意义，需要对公式做平滑处理。





流程总结

## 中文文本挖掘处理特点

* 需要分词

* 编码问题 unicode



## 数据收集

* 文本语料库
* 爬虫 ：通用的爬虫 beautifulsoup，主题爬虫（聚焦爬虫）ache



## 中文分词

* 引入停用词



## 特征处理







## 英文文本挖掘预处理特定

* 检查拼写：pyenchant 库
* 词干提取（stemming）和词形还原（lemmatization） ：单复数，时态；nltk库
* 大小写

















